{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel - Uncomment if needed\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install urllib\n",
    "# !{sys.executable} -m pip install requests\n",
    "# !{sys.executable} -m pip install bs4\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install selenium\n",
    "# !{sys.executable} -m pip install time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib.parse import urljoin\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url1(name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets all urls that are associated with a name of a municipality. As of now not needed.\n",
    "    \"\"\"\n",
    "    \n",
    "    e.send_keys(name)\n",
    "    e.send_keys(Keys.ENTER)\n",
    "    soup = BeautifulSoup(d.page_source)\n",
    "    d.find_element_by_id('suchfeld').clear()\n",
    "    time.sleep(0.7)\n",
    "    links = soup.find('tbody').find_all('a') \n",
    "    return [link.get('href') for link in links if 'Votemanager' not in link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url2(url1, date):\n",
    "    \n",
    "    \"\"\" To get the urls specific to the local elections.\n",
    "    It takes as input the overview url for each municipality from wahlen.regioit\n",
    "    and returns a list of all Ratswahl-url from its html text.\n",
    "    \"\"\"\n",
    "    \n",
    "    base = url1\n",
    "    r = requests.get(url1)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    kom_row  = [ r for r in soup.find_all('tr') if 'Kommunalwahlen' in r.text]\n",
    "    \n",
    "    res = [link.get('href') for link in BeautifulSoup(str(kom_row)).find_all('a') if date in link]\n",
    "    \n",
    "    if res:\n",
    "        href=res.pop()\n",
    "        return urljoin(base,href)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url3(url2):\n",
    "    r = requests.get(url2) \n",
    "    soup = BeautifulSoup(r.text)\n",
    "    base = url2\n",
    "    hrefs = [link.get('href') for link in soup.find_all('a')]\n",
    "    \n",
    "    matches = ['Stadtverordnetenwahl_Hessen', 'Gemeindewahl_Hessen']\n",
    "    \n",
    "    href_2=[href for href in hrefs if any(x in href for x in matches)]\n",
    "    \n",
    "    if href_2:\n",
    "        s = href_2.pop()\n",
    "        return urljoin(base,s)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webpage(data, col_city, col_link, year):\n",
    "    \n",
    "    \"\"\" Idea: Scrape html tables from the website for names and party affiliation.\n",
    "        Input: \n",
    "            Data = Dataframe to be used\n",
    "            col_city = Column where the name of the cities are stored\n",
    "            col_link = Column where the links are stored\n",
    "            year = Year of the relevant election\n",
    "            \n",
    "        Output:\n",
    "            output_data = Dataframe with scraped information\n",
    "    \n",
    "    Comments later: Ignore votes of candidates from previous crawler \"\"\"\n",
    "    \n",
    "    list_cities = data[col_city].unique()\n",
    "    iterations = len(data[col_link])\n",
    "    count = 0\n",
    "    data_url_01 = data[~data[col_link].isna()].reset_index(drop=True)\n",
    "    data_votes = pd.DataFrame()\n",
    "    data_votes_2 = pd.DataFrame()\n",
    "    output_data = pd.DataFrame()\n",
    "    \n",
    "    for n in range(len(data_url_01[col_link])):\n",
    "\n",
    "        print(f'City {n}/{iterations}')\n",
    "\n",
    "        r = requests.get(data_url_01[col_link][n])\n",
    "\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        \n",
    "        if soup.find_all(lambda tag: tag.name=='table') == []:\n",
    "            print(f'{list_cities[n]} has no entry')\n",
    "    \n",
    "        else:\n",
    "            table = soup.find_all(lambda tag: tag.name=='table')[-1]\n",
    "            rows_votes = table.find_all(lambda tag: tag.name=='abbr')\n",
    "            rows_votes = [row.get_text() for row in rows_votes]\n",
    "\n",
    "            if rows_votes == []:\n",
    "                print(f'{list_cities[n]} has no entry')\n",
    "\n",
    "            else:\n",
    "                data_votes = pd.DataFrame(data=[rows_votes]).T.rename({0:'candidate'}, axis=1)\n",
    "                #data_votes['votes'] = data_votes['candidate'].astype(str).str.replace('.','').str.extract(r'([0-9]+[^%])')\n",
    "\n",
    "                data_votes['candidate'] = data_votes['candidate'].astype(str).str.replace('ß', 'ss').str.replace('Ü', 'U').str.replace('ü', 'ue').str.replace('ä', 'ae').str.replace('ö', 'oe').str.replace('.', '').str.replace('é', 'e')\n",
    "                #data_votes['candidates'] = data_votes['candidate'].str.extract(r'([A-Z][a-z]+[\\s][A-Z][a-z]+[-]?[A-Z]*[a-z]*[,]*[\\s]*[A-Z]*[a-z]*)')\n",
    "\n",
    "                #data_votes_2['votes'] = data_votes[data_votes[\"votes\"].str.contains('<', na=False)].reset_index(drop=True)['votes'].str.replace('<', '')\n",
    "                #data_votes_2['candidates'] = data_votes['candidates'].dropna().reset_index(drop=True)\n",
    "                data_votes['city'] = list_cities[n]\n",
    "                data_votes['year'] = year\n",
    "                output_data = output_data.append(data_votes).dropna().reset_index(drop=True)\n",
    "    \n",
    "    return output_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
