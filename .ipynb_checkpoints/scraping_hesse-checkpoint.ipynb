{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel - Uncomment if needed\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install requests\n",
    "# !{sys.executable} -m pip install bs4\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install selenium\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install urllib\n",
    "# !{sys.executable} -m pip install re\n",
    "# !{sys.executable} -m pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import nbimporter\n",
    "\n",
    "# import functions from different scraping_util notebook; Make sure both scripts are in the same folder\n",
    "%run scraping_util.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gemeindename</th>\n",
       "      <th>plz</th>\n",
       "      <th>AGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>64283.0</td>\n",
       "      <td>06411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>Frankfurt am Main</td>\n",
       "      <td>60311.0</td>\n",
       "      <td>06412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>Offenbach am Main</td>\n",
       "      <td>63065.0</td>\n",
       "      <td>06413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>Wiesbaden</td>\n",
       "      <td>65183.0</td>\n",
       "      <td>06414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>Abtsteinach</td>\n",
       "      <td>69518.0</td>\n",
       "      <td>06431001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Gemeindename      plz       AGS\n",
       "2474          Darmstadt  64283.0  06411000\n",
       "2475  Frankfurt am Main  60311.0  06412000\n",
       "2476  Offenbach am Main  63065.0  06413000\n",
       "2477          Wiesbaden  65183.0  06414000\n",
       "2478        Abtsteinach  69518.0  06431001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data and filter for relevant BL\n",
    "data = pd.read_csv(r'C:\\Users\\mariu\\Documents\\pol_viol\\pol_viol\\liste_gemeinden_ger.csv', delimiter=';')\n",
    "data = data.dropna(thresh=8).reset_index(drop=True)\n",
    "data['AGS'] = '0' + data['Land'].astype(str) + data['RB'].astype(int).astype(str) + data['Kreis'].astype(int).astype(str) + '0'+ data['Gem'].astype(int).astype(str)\n",
    "data.loc[data['Gem'] < 10, 'AGS'] = '0' + data['Land'].astype(str) + data['RB'].astype(int).astype(str) + data['Kreis'].astype(int).astype(str) + '00' + data['Gem'].astype(int).astype(str)\n",
    "data_hessen= data[data['Land'] == 6]\n",
    "data_scrape = data_hessen[['Gemeindename', 'plz', 'AGS']]\n",
    "data_scrape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# set up webdriver and search field element\n",
    "# regularly check for the right version of webdriver\n",
    "# chrome version: 97.0.4692.99\n",
    "# chrome driver 97.0.4692.71\n",
    "# Important to keep in mind: both need to be of version 97.\n",
    "d=webdriver.Chrome(r\"C:\\Users\\mariu\\Documents\\pol_viol\\pol_viol\\chromedriver.exe\")\n",
    "d.get('https://wahlen.votemanager.de/#')\n",
    "e = d.find_element_by_id('suchfeld')\n",
    "\n",
    "# search for each municipality and get the url to the municipality specific overview page\n",
    "data_scrape['url'] = data_scrape['Gemeindename'].apply(get_url1)\n",
    "data_scrape.reset_index(drop=True, inplace=True)\n",
    "data_scrape = data_scrape[data_scrape.astype(str)['url']!='[]'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scrape['results'] = data_scrape['url'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the relevant link and ignore mismatches\n",
    "for row in data_scrape.iterrows():\n",
    "    if row[1][4] > 1:\n",
    "        for link in row[1][3]:\n",
    "            if row[1][2] in link:\n",
    "                data_scrape.loc[(data_scrape['Gemeindename'] == row[1][0]), 'url'] = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Create links for the three last election cycles\n",
    "data_scrape['url_copy'] = data_scrape['url'].astype(str)\n",
    "data_exp = data_scrape['url_copy'].str.replace(\"\\['\", \"\").str.replace(\"'\\]\", \"\").str.split('/',expand=True)\n",
    "data_scrape['url_21'] = data_exp[0]+ '//' + data_exp[2]+ '/' + '2021-03-14/' + data_exp[3] + '/html5/' + data_exp[4]\n",
    "data_scrape['url_16'] = data_exp[0]+ '//' + data_exp[2]+ '/' + '20160306/' + data_exp[3] + '/html5/' + data_exp[4]\n",
    "data_scrape['url_11'] = data_exp[0]+ '//' + data_exp[2]+ '/' + '20110327/' + data_exp[3] + '/html5/' + data_exp[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url3(url2):\n",
    "    \"\"\" Function to get three possible links for every election. Each of 'matches' defines a possible link \n",
    "        specification. Only input is the link column.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = requests.get(url2) \n",
    "    soup = BeautifulSoup(r.text)\n",
    "    base = url2\n",
    "    hrefs = [link.get('href') for link in soup.find_all('a')]\n",
    "    \n",
    "    matches = ['Stadtverordnetenwahl_Hessen', 'Gemeindewahl_Hessen', 'Gemeindewahl-Hessen']\n",
    "    \n",
    "    href_2=[href for href in hrefs if any(x in href for x in matches)]\n",
    "    \n",
    "    if href_2:\n",
    "        s = href_2.pop()\n",
    "        return urljoin(base,s)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some invalid links -> get rid of them\n",
    "data_scrape = data_scrape[~data_scrape['url_21'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_scrape['url_21'] = data_scrape['url_21'].apply(get_url3)\n",
    "data_scrape['url_16'] = data_scrape['url_16'].apply(get_url3)\n",
    "data_scrape['url_11'] = data_scrape['url_11'].apply(get_url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_21 = scrape_webpage(data_scrape, 'Gemeindename', 'url_21', 2021)\n",
    "data_16 = scrape_webpage(data_scrape, 'Gemeindename', 'url_16', 2016)\n",
    "data_11 = scrape_webpage(data_scrape, 'Gemeindename', 'url_11', 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hessen = data_21.append(data_16)\n",
    "data_hessen = data_hessen.append(data_11)\n",
    "data_hessen.reset_index(drop=True, inplace=True)\n",
    "data_hessen['candidates_name'] = data_hessen['candidate'].str.split(',', expand=True)[0]\n",
    "data_hessen['party'] = data_hessen['candidate'].str.split(',', expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hessen = data_hessen[~data_hessen['party'].isna()]\n",
    "data_hessen['candidates_name'] = data_hessen['candidates_name'].str.replace('Prof ', '').str.replace('habil ', '').str.replace('Dr med ', '').str.replace('Dr Dr ', '').str.replace('Dr ', '').str.replace('dent ', '').str.replace('rer nat ', '').str.replace('PD ', '')\n",
    "data_hessen['first_name'] = data_hessen['candidates_name'].str.split(' ', expand=True)[0]\n",
    "data_hessen['BL'] = 'HE'\n",
    "data_hessen_2 = data_hessen[(data_hessen['party'].str.contains('CDU')) | (data_hessen['party'].str.contains('SPD')) | (data_hessen['party'].str.contains('AfD')) | (data_hessen['party'].str.contains('GRUNE')) | ((data_hessen['party'].str.contains('FDP'))) | ((data_hessen['party'].str.contains('LINKE')))]\n",
    "data_hessen_2.reset_index(drop=True, inplace=True)\n",
    "data_hessen_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential new observations for analysis\n",
    "count = 0\n",
    "data_hessen_3 = data_hessen_2[data_hessen_2['year'] != 2011]\n",
    "for city in data_hessen_3['city'].unique():\n",
    "    count += len(data_hessen_3[data_hessen_3['city'] == city]['party'].unique())\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#data_hessen_2.to_csv(r'C:\\Users\\mariu\\Documents\\pol_viol\\pol_viol\\scraped_hessen.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
